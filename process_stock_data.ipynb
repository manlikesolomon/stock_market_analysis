{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397ee3f8",
   "metadata": {},
   "source": [
    "## Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "134f13fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/homebrew/Cellar/apache-spark/4.0.0/libexec'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import Window\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "987ade59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/30 19:13:48 WARN Utils: Your hostname, Solomons-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 172.16.1.16 instead (on interface en0)\n",
      "25/07/30 19:13:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/opt/homebrew/Cellar/apache-spark/4.0.0/libexec/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/solomon.balogun/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/solomon.balogun/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-159954af-07f6-44f9-8be9-dd1fad41dcf9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 89ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-159954af-07f6-44f9-8be9-dd1fad41dcf9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/2ms)\n",
      "25/07/30 19:13:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "builder = SparkSession.builder \\\n",
    "    .appName(\"DeltaLake4App\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f870a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.1.16:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DeltaLake4App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1276eb040>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c10eaa",
   "metadata": {},
   "source": [
    "## Get and Process Stock Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "067fb029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/kdtrw_0n6t1cplgyjkc4hwx00000gr/T/ipykernel_81933/2246109638.py:15: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  raw = yf.download(tickers, start=start_date, end=end_date, group_by='ticker')\n",
      "[*********************100%***********************]  30 of 30 completed\n"
     ]
    }
   ],
   "source": [
    "# Get yesterday's date\n",
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "end_date = yesterday.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Get start date (3 years before yesterday)\n",
    "start_date = (yesterday - timedelta(days=3*365)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "tickers = [\n",
    "    \"AAPL\", \"MSFT\", \"NVDA\", \"GOOGL\", \"AMZN\", \"META\", \"AVGO\", \"BRK-B\", \"TSLA\", \"TSM\",\n",
    "    \"JPM\", \"WMT\", \"LLY\", \"ORCL\", \"V\", \"MA\", \"NFLX\", \"XOM\", \"COST\", \"JNJ\",\n",
    "    \"ABBV\", \"SAP\", \"BABA\", \"GS\", \"HD\", \"VRTX\", \"UNH\", \"MRK\", \"PEP\", \"TMO\"\n",
    "]\n",
    "\n",
    "# Download stock data\n",
    "raw = yf.download(tickers, start=start_date, end=end_date, group_by='ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "817e92f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Price</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9312</th>\n",
       "      <td>LLY</td>\n",
       "      <td>2023-10-26</td>\n",
       "      <td>573.012208</td>\n",
       "      <td>574.000160</td>\n",
       "      <td>559.338925</td>\n",
       "      <td>561.255554</td>\n",
       "      <td>2846500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12801</th>\n",
       "      <td>XOM</td>\n",
       "      <td>2022-10-12</td>\n",
       "      <td>88.823090</td>\n",
       "      <td>90.107612</td>\n",
       "      <td>88.577122</td>\n",
       "      <td>89.670326</td>\n",
       "      <td>12635800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14746</th>\n",
       "      <td>JNJ</td>\n",
       "      <td>2024-07-23</td>\n",
       "      <td>148.472470</td>\n",
       "      <td>148.656477</td>\n",
       "      <td>146.787384</td>\n",
       "      <td>147.542770</td>\n",
       "      <td>6261900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8804</th>\n",
       "      <td>WMT</td>\n",
       "      <td>2024-10-14</td>\n",
       "      <td>79.462181</td>\n",
       "      <td>79.918802</td>\n",
       "      <td>79.174310</td>\n",
       "      <td>79.700417</td>\n",
       "      <td>9914500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13907</th>\n",
       "      <td>COST</td>\n",
       "      <td>2024-03-14</td>\n",
       "      <td>733.355267</td>\n",
       "      <td>735.689409</td>\n",
       "      <td>724.078172</td>\n",
       "      <td>727.018250</td>\n",
       "      <td>1826100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5521</th>\n",
       "      <td>BRK-B</td>\n",
       "      <td>2023-08-29</td>\n",
       "      <td>355.040009</td>\n",
       "      <td>358.589996</td>\n",
       "      <td>354.010010</td>\n",
       "      <td>358.290009</td>\n",
       "      <td>2285600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19796</th>\n",
       "      <td>UNH</td>\n",
       "      <td>2023-10-04</td>\n",
       "      <td>494.294171</td>\n",
       "      <td>496.554694</td>\n",
       "      <td>491.500063</td>\n",
       "      <td>495.390472</td>\n",
       "      <td>2801700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16093</th>\n",
       "      <td>SAP</td>\n",
       "      <td>2023-12-11</td>\n",
       "      <td>154.570297</td>\n",
       "      <td>157.207205</td>\n",
       "      <td>154.570297</td>\n",
       "      <td>157.157837</td>\n",
       "      <td>1078700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16576</th>\n",
       "      <td>BABA</td>\n",
       "      <td>2022-11-16</td>\n",
       "      <td>77.568774</td>\n",
       "      <td>77.694319</td>\n",
       "      <td>74.758455</td>\n",
       "      <td>75.482765</td>\n",
       "      <td>26637300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>HD</td>\n",
       "      <td>2023-07-25</td>\n",
       "      <td>309.284957</td>\n",
       "      <td>311.026081</td>\n",
       "      <td>308.105150</td>\n",
       "      <td>309.342041</td>\n",
       "      <td>2890800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price Ticker       Date        Open        High         Low       Close  \\\n",
       "9312     LLY 2023-10-26  573.012208  574.000160  559.338925  561.255554   \n",
       "12801    XOM 2022-10-12   88.823090   90.107612   88.577122   89.670326   \n",
       "14746    JNJ 2024-07-23  148.472470  148.656477  146.787384  147.542770   \n",
       "8804     WMT 2024-10-14   79.462181   79.918802   79.174310   79.700417   \n",
       "13907   COST 2024-03-14  733.355267  735.689409  724.078172  727.018250   \n",
       "5521   BRK-B 2023-08-29  355.040009  358.589996  354.010010  358.290009   \n",
       "19796    UNH 2023-10-04  494.294171  496.554694  491.500063  495.390472   \n",
       "16093    SAP 2023-12-11  154.570297  157.207205  154.570297  157.157837   \n",
       "16576   BABA 2022-11-16   77.568774   77.694319   74.758455   75.482765   \n",
       "18246     HD 2023-07-25  309.284957  311.026081  308.105150  309.342041   \n",
       "\n",
       "Price    Volume  \n",
       "9312    2846500  \n",
       "12801  12635800  \n",
       "14746   6261900  \n",
       "8804    9914500  \n",
       "13907   1826100  \n",
       "5521    2285600  \n",
       "19796   2801700  \n",
       "16093   1078700  \n",
       "16576  26637300  \n",
       "18246   2890800  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert multi-index DataFrame to flat DataFrame\n",
    "df_list = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    ticker_df = raw[ticker].reset_index()\n",
    "    ticker_df['Ticker'] = ticker\n",
    "    df_list.append(ticker_df)\n",
    "\n",
    "flat_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# reorder columns \n",
    "cols = ['Ticker', 'Date'] + [col for col in flat_df.columns if col not in ['Ticker', 'Date']]\n",
    "flat_df = flat_df[cols]\n",
    "\n",
    "flat_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4001372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ticker: string (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# store data in a spark dataframe\n",
    "spark_df = spark.createDataFrame(flat_df)\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a16f189",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36678ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate daily returns\n",
    "window_spec = Window.partitionBy('Ticker').orderBy('Date')\n",
    "\n",
    "spark_df = spark_df.withColumn('Prev_close', lag('Close').over(window_spec))\n",
    "\n",
    "spark_df = spark_df.withColumn(\n",
    "    'Daily_return',\n",
    "    round(((col('Close') - col('Prev_close')) / col('Prev_close') * 100), 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43fc41b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate daily volutility rate \n",
    "spark_df = spark_df.withColumn(\n",
    "    'Volatility',\n",
    "    round(((col('High') - col('Low')) / col('Low') * 100), 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4afdb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate moving averages \n",
    "\n",
    "ma_window_12 = Window.partitionBy('Ticker').orderBy('Date').rowsBetween(-11, 0)\n",
    "\n",
    "ma_window_26 = Window.partitionBy('Ticker').orderBy('Date').rowsBetween(-25, 0)\n",
    "\n",
    "ma_window_50 = Window.partitionBy('Ticker').orderBy('Date').rowsBetween(-49, 0)\n",
    "\n",
    "ma_window_200 = Window.partitionBy('Ticker').orderBy('Date').rowsBetween(-199, 0)\n",
    "\n",
    "\n",
    "spark_df = spark_df.withColumn(\n",
    "    'MA_12',\n",
    "    round(avg('Close').over(ma_window_12), 2)\n",
    ").withColumn(\n",
    "    'MA_26',\n",
    "    round(avg('Close').over(ma_window_26), 2)\n",
    ").withColumn(\n",
    "    'MA_50',\n",
    "    round(avg('Close').over(ma_window_50), 2)\n",
    ").withColumn(\n",
    "    'MA_200',\n",
    "    round(avg('Close').over(ma_window_200), 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11bab176",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_cum = Window.partitionBy(\"Ticker\").orderBy(\"Date\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "spark_df = spark_df.withColumn(\"First_Close\", first(\"Close\").over(window_cum))\n",
    "spark_df = spark_df.withColumn(\"Cumulative_Return\", round(((col(\"Close\") - col(\"First_Close\")) / col(\"First_Close\")) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4bf7f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn(\"Close_7_Days_Ago\", lag(\"Close\", 7).over(window_spec))\n",
    "spark_df = spark_df.withColumn(\n",
    "    \"Momentum_7d\", \n",
    "    round(((col(\"Close\") - col(\"Close_7_Days_Ago\")) / col(\"Close_7_Days_Ago\")) * 100, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1324504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute MACD and signal lines \n",
    "spark_df = spark_df.withColumn(\n",
    "    'MACD',\n",
    "    col('MA_12') - col('MA_26')\n",
    ")\n",
    "\n",
    "signal_window = Window.partitionBy('Ticker').orderBy('Date').rowsBetween(-8, 0)\n",
    "spark_df = spark_df.withColumn(\n",
    "    'Signal_line',\n",
    "    round(avg(col('MACD')).over(signal_window), 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b18f0d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute draw down\n",
    "# measure draw from peak close price\n",
    "\n",
    "peak_close = max(col('Close')).over(Window.partitionBy('Ticker').orderBy('Date').rowsBetween(Window.unboundedPreceding, 0))\n",
    "\n",
    "spark_df = spark_df.withColumn(\n",
    "    'DrawDown',\n",
    "    round(((col('Close') - peak_close) / peak_close * 100), 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ba99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b0dae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d31cec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
